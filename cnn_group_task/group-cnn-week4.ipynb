{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:42:59.227658Z",
     "start_time": "2025-04-08T19:42:54.323124Z"
    }
   },
   "source": [
    "# NGUYEN CHI MANH\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', train=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.MNIST(root='../../data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "class CNN_dynamic(nn.Module):\n",
    "    def __init__(self, kernel_size, conv_layer_num):\n",
    "        super(CNN_dynamic, self).__init__()\n",
    "        feature_size = 28\n",
    "        conv_layers = []\n",
    "\n",
    "        conv_layers.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernel_size, stride=1, padding=kernel_size//2))\n",
    "        conv_layers.append(nn.ReLU())\n",
    "        for i in range(conv_layer_num - 1):\n",
    "            conv_layers.append(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=kernel_size, stride=1, padding=kernel_size//2))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "            if i < conv_layer_num - 2:\n",
    "                conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "                feature_size //= 2\n",
    "            conv_layers.append(nn.Dropout(0.25))\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=32 * feature_size * feature_size, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv_layers(input)\n",
    "        output = self.fc_layers(output)\n",
    "        return output\n",
    "\n",
    "def train(model, device, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct_predict = 0\n",
    "\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = output.max(1)\n",
    "        correct_predict += predicted.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return 100 * correct_predict / total\n",
    "\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct_predict = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_index, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct_predict += predicted.eq(target).sum().item()\n",
    "\n",
    "    return 100 * correct_predict/total\n",
    "\n",
    "def main():\n",
    "    kernel_sizes = [3,5,7]\n",
    "    num_conv_layers = [3, 4, 5]\n",
    "    lrs = [0.1, 0.01, 0.001, 0.0001]\n",
    "    num_epochs = 3\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for num_conv_layer in num_conv_layers:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for lr in lrs:\n",
    "\n",
    "                config_key = f\"L{num_conv_layer}_K{kernel_size}_LR{lr}\"\n",
    "                results[config_key] = {\n",
    "                    'train_acc': [], 'test_acc': [],\n",
    "                    'params': {'layers': num_conv_layer, 'kernel': kernel_size, 'lr': lr}\n",
    "                }\n",
    "\n",
    "                cnn_dynamic = CNN_dynamic(kernel_size, num_conv_layer).to(device)\n",
    "                optimizer = optim.Adam(cnn_dynamic.parameters(), lr)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    train_acc = train(cnn_dynamic, device, train_loader, optimizer, loss_fn)\n",
    "                    test_acc = test(cnn_dynamic, device, val_loader, loss_fn)\n",
    "\n",
    "                    train_accuracies.append(train_acc)\n",
    "                    test_accuracies.append(test_acc)\n",
    "\n",
    "                    results[config_key]['train_acc'].append(train_acc)\n",
    "                    results[config_key]['test_acc'].append(test_acc)\n",
    "\n",
    "                    if test_acc > best_accuracy:\n",
    "                        best_accuracy = test_acc\n",
    "                        best_params = {'num_con_layer':num_conv_layer,'kernel_size': kernel_size, 'lr': lr }\n",
    "                        \n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "                    print(f\"Config: {config_key}\")\n",
    "                \n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.plot(range(1, num_epochs+1), results[config_key]['train_acc'], '-o', label='Train')\n",
    "                plt.plot(range(1, num_epochs+1), results[config_key]['test_acc'], '-s', label='Validation')\n",
    "                plt.title(f\"Layers={num_conv_layer}, Kernel={kernel_size}, LR={lr}\")\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Accuracy (%)')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'accuracy_L{num_conv_layer}_K{kernel_size}_LR{lr}.png')\n",
    "                plt.close()\n",
    "\n",
    "    best_key = f\"L{best_params['layers']}_K{best_params['kernel']}_LR{best_params['lr']}\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs+1), results[best_key]['train_acc'], '-bo', label='Train')\n",
    "    plt.plot(range(1, num_epochs+1), results[best_key]['test_acc'], '-ro', label='Validation')\n",
    "    plt.title(f\"Best Model: {best_params}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best accuracy: {best_accuracy:.2f}%\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m      9\u001B[39m device = torch.device(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     11\u001B[39m transform = transforms.Compose([\n\u001B[32m     12\u001B[39m     transforms.ToTensor(),\n\u001B[32m     13\u001B[39m     transforms.Normalize((\u001B[32m0.1307\u001B[39m,), (\u001B[32m0.3081\u001B[39m,))\n\u001B[32m     14\u001B[39m ])\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m train_dataset = \u001B[43mtorchvision\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdatasets\u001B[49m\u001B[43m.\u001B[49m\u001B[43mMNIST\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m../../data\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m val_dataset = torchvision.datasets.MNIST(root=\u001B[33m'\u001B[39m\u001B[33m../../data\u001B[39m\u001B[33m'\u001B[39m, train=\u001B[38;5;28;01mFalse\u001B[39;00m, transform=transform)\n\u001B[32m     19\u001B[39m train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=\u001B[32m64\u001B[39m, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torchvision/datasets/mnist.py:103\u001B[39m, in \u001B[36mMNIST.__init__\u001B[39m\u001B[34m(self, root, train, transform, target_transform, download)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m.download()\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._check_exists():\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mDataset not found. You can use download=True to download it\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    105\u001B[39m \u001B[38;5;28mself\u001B[39m.data, \u001B[38;5;28mself\u001B[39m.targets = \u001B[38;5;28mself\u001B[39m._load_data()\n",
      "\u001B[31mRuntimeError\u001B[39m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:42:45.608754Z",
     "start_time": "2025-04-08T19:42:45.518030Z"
    }
   },
   "source": [
    "main()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mmain\u001B[49m()\n",
      "\u001B[31mNameError\u001B[39m: name 'main' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
